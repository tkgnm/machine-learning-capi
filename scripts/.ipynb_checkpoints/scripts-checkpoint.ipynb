{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa08d12-9f4f-49c2-8e10-d1c3f5690e2e",
   "metadata": {},
   "source": [
    "Order for scripts:\n",
    "\n",
    "1. download.py\n",
    "2. import_transform.py\n",
    "3. matrix.py\n",
    "4. run.py\n",
    "\n",
    "Extra scripts useful for AWS and to be experimented with\n",
    "\n",
    "1. _import_and_merge.py\n",
    "2. _join.py\n",
    "3. _split.py\n",
    "4. _tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05062916-93d2-4e92-aa35-71f7925225d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np   \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85063d0d-9af3-4507-bb81-250532393d62",
   "metadata": {},
   "source": [
    "## download.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0da1179-c70f-4d45-8b87-7daeadad3829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running\n",
      "Fetching response 1\n",
      "Writing to disk...\n",
      "Saved articledata/articles_1.json\n",
      "Fetching response 2\n",
      "Writing to disk...\n",
      "Saved articledata/articles_2.json\n",
      "Fetching response 3\n",
      "Writing to disk...\n",
      "Saved articledata/articles_3.json\n",
      "Fetching response 4\n",
      "Writing to disk...\n",
      "Saved articledata/articles_4.json\n",
      "Fetching response 5\n",
      "Writing to disk...\n",
      "Saved articledata/articles_5.json\n",
      "Fetching response 6\n",
      "Writing to disk...\n",
      "Saved articledata/articles_6.json\n",
      "Fetching response 7\n",
      "Writing to disk...\n",
      "Saved articledata/articles_7.json\n",
      "Fetching response 8\n",
      "Writing to disk...\n",
      "Saved articledata/articles_8.json\n",
      "Fetching response 9\n",
      "Writing to disk...\n",
      "Saved articledata/articles_9.json\n",
      "Fetching response 10\n",
      "Writing to disk...\n",
      "Saved articledata/articles_10.json\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Running\")\n",
    "\n",
    "for page_no in range(1,11): #gets 10 pages worth of CAPI data\n",
    "    print(f'Fetching response {page_no}')\n",
    "    \n",
    "    url = f'https://content.guardianapis.com/search?page-size=200&page={page_no}&show-fields=trailText,headline,body,byline&api-key=f2652ec5-7f11-4682-bb81-16c0a5e6c850'\n",
    "          \n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "          \n",
    "    print('Writing to disk...')\n",
    "    open(f'articledata/articles_{page_no}.json', 'wb').write(r.content)\n",
    "    print(f'Saved articledata/articles_{page_no}.json')\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab18be5-ba28-495d-ab97-ddf37ef24fe4",
   "metadata": {},
   "source": [
    "## import_transform.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a9f6919-5839-4b92-8f51-b8231e8b398c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening articles_1.json\n",
      "Appending items to array\n",
      "Opening articles_2.json\n",
      "Appending items to array\n",
      "Opening articles_3.json\n",
      "Appending items to array\n",
      "Opening articles_4.json\n",
      "Appending items to array\n",
      "Opening articles_5.json\n",
      "Appending items to array\n",
      "Opening articles_6.json\n",
      "Appending items to array\n",
      "Opening articles_7.json\n",
      "Appending items to array\n",
      "Opening articles_8.json\n",
      "Appending items to array\n",
      "Opening articles_9.json\n",
      "Appending items to array\n",
      "Opening articles_10.json\n",
      "Appending items to array\n",
      "Appending items to array\n",
      "Removing html\n",
      "Performing vectorization\n",
      "Saving data\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "files = [\"articles_1.json\", \"articles_2.json\", \"articles_3.json\", \"articles_4.json\", \"articles_5.json\", \"articles_6.json\", \"articles_7.json\", \"articles_8.json\", \"articles_9.json\", \"articles_10.json\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    print(f'Opening {file}')\n",
    "        \n",
    "    # Opening JSON file\n",
    "    f = open(\"articledata/\" + file)\n",
    "    \n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    # print(data)\n",
    "    data = json.load(f)\n",
    "    \n",
    "    # print((data[\"response\"][\"results\"][0]))\n",
    "    \n",
    "    data = data[\"response\"][\"results\"]\n",
    "    \n",
    "    print(\"Appending items to array\")\n",
    "    for item in data:\n",
    "        results.append(item)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "myDict = {\"all_articles\": results}\n",
    "\n",
    "data = myDict[\"all_articles\"]\n",
    "\n",
    "headline = []\n",
    "trailText = []\n",
    "body = []\n",
    "ids = []\n",
    "\n",
    "print(\"Appending items to array\")\n",
    "for article in data:\n",
    "\n",
    "    headline.append(article[\"fields\"][\"headline\"])\n",
    "    trailText.append(article[\"fields\"][\"trailText\"])\n",
    "    body.append(article[\"fields\"][\"body\"])\n",
    "    ids.append(article[\"id\"])\n",
    "    \n",
    "\n",
    "print(\"Removing html\")\n",
    "htmlRemover = re.compile('<.*?>') \n",
    "newlineRemover = '\\n'\n",
    "\n",
    "# as per recommendation from @freylis, compile once only\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleantext = re.sub(htmlRemover, '', raw_html)\n",
    "    cleantext = re.sub(newlineRemover, '', cleantext)\n",
    "    return cleantext\n",
    "\n",
    "cleanTrailText = []\n",
    "cleanBody = []\n",
    "\n",
    "for i in range(len(body)):\n",
    "    cleanTrailText.append(cleanhtml(trailText[i]))\n",
    "    cleanBody.append(cleanhtml(body[i]))\n",
    "    \n",
    "\n",
    "print(\"Performing vectorization\")\n",
    "\n",
    "vect = TfidfVectorizer(min_df=1, stop_words=\"english\")\n",
    "tfidf = vect.fit_transform(cleanBody)                                                                                       \n",
    "pairwise_similarity = tfidf * tfidf.T \n",
    "\n",
    "arr = pairwise_similarity.toarray()     \n",
    "np.fill_diagonal(arr, np.nan)     \n",
    "arr = arr.astype('float16') #compress to float 16\n",
    "print(\"Saving data\")\n",
    "\n",
    "###before you save, split the data and put it back together in the run.py\n",
    "\n",
    "# np.split(arr, 2)\n",
    "\n",
    "# file = 1\n",
    "# for split in (np.split(arr, 2)):\n",
    "#     print(f\"Saving file {file}\")\n",
    "#     np.save(f\"data/sparseMatrix{file}.npy\", split)\n",
    "#     file += 1\n",
    "\n",
    "np.save(\"data/sparseMatrix.npy\", arr)\n",
    "\n",
    "textfile = open(\"data/article_ids.txt\", \"w\")\n",
    "for element in ids:\n",
    "    textfile.write(element + \"\\n\")\n",
    "textfile.close()\n",
    "    \n",
    "print(\"Finished\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f2ad19-cb7c-4784-9f0d-6e4e147fb6c4",
   "metadata": {},
   "source": [
    "# run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfe5e8f0-a30f-4eb1-a08b-c7b0174aeec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE FOR LATER ARTICLE: money/2022/aug/23/six-ways-to-reduce-gas-consumption-uk-energy-bills\n",
      "money/2022/aug/23/six-ways-to-reduce-gas-consumption-uk-energy-bills\n",
      "\n",
      "\n",
      "RECOMMENDED ARTICLE: environment/2022/aug/23/energy-use-is-a-decision-for-individuals-insist-no-10-and-truss-allies\n",
      "environment/2022/aug/23/energy-use-is-a-decision-for-individuals-insist-no-10-and-truss-allies\n",
      "Similarity score: 0.427001953125\n"
     ]
    }
   ],
   "source": [
    "### load data\n",
    "\n",
    "txt_file = open(\"data/article_ids.txt\", \"r\")\n",
    "file_content = txt_file.read()\n",
    "\n",
    "ids = file_content.split(\"\\n\")\n",
    "txt_file.close()\n",
    "\n",
    "arr = np.load('data/sparseMatrix.npy')\n",
    "\n",
    "### return id\n",
    "\n",
    "#example ID\n",
    "ex = ids[random.randint(0, len(ids))] #random number generator\n",
    "\n",
    "#get index positoin of article\n",
    "input_idx = ids.index(ex)                    \n",
    "result_idx = np.nanargmax(arr[input_idx]) \n",
    "\n",
    "print(\"SAVE FOR LATER ARTICLE: {}\".format(ids[input_idx]))\n",
    "print(\"{}\".format(ids[input_idx]))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"RECOMMENDED ARTICLE: {}\".format(ids[result_idx]))\n",
    "print(\"{}\".format(ids[result_idx]))\n",
    "similarity_score = round(arr[input_idx,result_idx], 3)\n",
    "print(\"Similarity score: {}\".format(similarity_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a993da37-0d81-4cf9-b896-7a8a21d909aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_capi",
   "language": "python",
   "name": "ml_capi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
